"""Graph-related routes for the text2sql API."""

import asyncio
import json
import logging
import time

from fastapi import APIRouter, Request, HTTPException, UploadFile, File
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel

from api.agents import AnalysisAgent, RelevancyAgent, ResponseFormatterAgent
from api.auth.user_management import token_required
from api.extensions import db
from api.graph import find, get_db_description
from api.loaders.csv_loader import CSVLoader
from api.loaders.json_loader import JSONLoader
from api.loaders.postgres_loader import PostgresLoader
from api.loaders.mysql_loader import MySQLLoader
from api.loaders.odata_loader import ODataLoader

# Use the same delimiter as in the JavaScript
MESSAGE_DELIMITER = "|||FALKORDB_MESSAGE_BOUNDARY|||"

graphs_router = APIRouter()


class GraphData(BaseModel):
    """Graph data model.

    Args:
        BaseModel (_type_): _description_
    """
    database: str


class ChatRequest(BaseModel):
    """Chat request model.

    Args:
        BaseModel (_type_): _description_
    """
    chat: list
    result: list = None
    instructions: str = None


class ConfirmRequest(BaseModel):
    """Confirmation request model.

    Args:
        BaseModel (_type_): _description_
    """
    sql_query: str
    confirmation: str = ""
    chat: list = []


def get_database_type_and_loader(db_url: str):
    """
    Determine the database type from URL and return appropriate loader class.
    
    Args:
        db_url: Database connection URL
        
    Returns:
        tuple: (database_type, loader_class)
    """
    if not db_url or db_url == "No URL available for this database.":
        return None, None

    db_url_lower = db_url.lower()

    if db_url_lower.startswith('postgresql://') or db_url_lower.startswith('postgres://'):
        return 'postgresql', PostgresLoader
    elif db_url_lower.startswith('mysql://'):
        return 'mysql', MySQLLoader
    else:
        # Default to PostgresLoader for backward compatibility
        return 'postgresql', PostgresLoader

def sanitize_query(query: str) -> str:
    """Sanitize the query to prevent injection attacks."""
    return query.replace('\n', ' ').replace('\r', ' ')[:500]

def sanitize_log_input(value: str) -> str:
    """Sanitize input for safe logging (remove newlines and carriage returns)."""
    if not isinstance(value, str):
        return str(value)
    return value.replace('\n', ' ').replace('\r', ' ')

@graphs_router.get("")
@token_required
async def list_graphs(request: Request):
    """
    This route is used to list all the graphs that are available in the database.
    """
    user_id = request.state.user_id
    user_graphs = await db.list_graphs()
    # Only include graphs that start with user_id + '_', and strip the prefix
    filtered_graphs = [graph[len(f"{user_id}_"):]
                       for graph in user_graphs if graph.startswith(f"{user_id}_")]
    return JSONResponse(content=filtered_graphs)


@graphs_router.get("/{graph_id}/data")
@token_required
async def get_graph_data(request: Request, graph_id: str):
    """Return all nodes and edges for the specified graph (namespaced to the user).

    This endpoint returns a JSON object with two keys: `nodes` and `edges`.
    Nodes contain a minimal set of properties (id, name, labels, props).
    Edges contain source and target node names (or internal ids), type and props.
    """
    if not graph_id or not isinstance(graph_id, str):
        return JSONResponse(content={"error": "Invalid graph_id"}, status_code=400)

    graph_id = graph_id.strip()[:200]
    namespaced = request.state.user_id + "_" + graph_id

    try:
        graph = db.select_graph(namespaced)
    except Exception as e:
        logging.error("Failed to select graph %s: %s", sanitize_log_input(namespaced), e)
        return JSONResponse(content={"error": "Graph not found or database error"}, status_code=404)

    # Build table nodes with columns and table-to-table links (foreign keys)
    tables_query = """
    MATCH (t:Table)
    OPTIONAL MATCH (c:Column)-[:BELONGS_TO]->(t)
    RETURN t.name AS table, collect(DISTINCT {name: c.name, type: c.type}) AS columns
    """

    links_query = """
    MATCH (src_col:Column)-[:BELONGS_TO]->(src_table:Table),
          (tgt_col:Column)-[:BELONGS_TO]->(tgt_table:Table),
          (src_col)-[:REFERENCES]->(tgt_col)
    RETURN DISTINCT src_table.name AS source, tgt_table.name AS target
    """

    try:
        tables_res = (await graph.query(tables_query)).result_set
        links_res = (await graph.query(links_query)).result_set
    except Exception as e:
        logging.error("Error querying graph data for %s: %s", sanitize_log_input(namespaced), e)
        return JSONResponse(content={"error": "Failed to read graph data"}, status_code=500)

    nodes = []
    for row in tables_res:
        try:
            table_name, columns = row
        except Exception:
            continue
        # Normalize columns: ensure a list of dicts with name/type
        if not isinstance(columns, list):
            columns = [] if columns is None else [columns]

        normalized = []
        for col in columns:
            try:
                # col may be a mapping-like object or a simple value
                if not col:
                    continue
                # Some drivers may return a tuple or list for the collected map
                if isinstance(col, (list, tuple)) and len(col) >= 2:
                    # try to interpret as (name, type)
                    name = col[0]
                    ctype = col[1] if len(col) > 1 else None
                elif isinstance(col, dict):
                    name = col.get('name') or col.get('columnName')
                    ctype = col.get('type') or col.get('dataType')
                else:
                    name = str(col)
                    ctype = None

                if not name:
                    continue

                normalized.append({"name": name, "type": ctype})
            except Exception:
                continue

        nodes.append({
            "id": table_name,
            "name": table_name,
            "columns": normalized,
        })

    links = []
    seen = set()
    for row in links_res:
        try:
            source, target = row
        except Exception:
            continue
        key = (source, target)
        if key in seen:
            continue
        seen.add(key)
        links.append({"source": source, "target": target})

    return JSONResponse(content={"nodes": nodes, "links": links})


@graphs_router.post("")
@token_required
async def load_graph(request: Request, data: GraphData = None, file: UploadFile = File(None)):
    """
    This route is used to load the graph data into the database.
    It expects either:
    - A JSON payload (application/json)
    - A File upload (multipart/form-data)
    - An XML payload (application/xml or text/xml)
    """
    success, result = False, "Invalid content type"
    graph_id = ""

    # ‚úÖ Handle JSON Payload
    if data:
        if not hasattr(data, 'database') or not data.database:
            raise HTTPException(status_code=400, detail="Invalid JSON data")

        graph_id = request.state.user_id + "_" + data.database
        success, result = await JSONLoader.load(graph_id, data.dict())

    # ‚úÖ Handle File Upload
    elif file:
        content = await file.read()
        filename = file.filename

        # ‚úÖ Check if file is JSON
        if filename.endswith(".json"):
            try:
                data = json.loads(content.decode("utf-8"))
                graph_id = request.state.user_id + "_" + data.get("database", "")
                success, result = await JSONLoader.load(graph_id, data)
            except json.JSONDecodeError:
                raise HTTPException(status_code=400, detail="Invalid JSON file")

        # ‚úÖ Check if file is XML
        elif filename.endswith(".xml"):
            xml_data = content.decode("utf-8")
            graph_id = request.state.user_id + "_" + filename.replace(".xml", "")
            success, result = await ODataLoader.load(graph_id, xml_data)

        # ‚úÖ Check if file is csv
        elif filename.endswith(".csv"):
            csv_data = content.decode("utf-8")
            graph_id = request.state.user_id + "_" + filename.replace(".csv", "")
            success, result = await CSVLoader.load(graph_id, csv_data)

        else:
            raise HTTPException(status_code=415, detail="Unsupported file type")
    else:
        raise HTTPException(status_code=415, detail="Unsupported Content-Type")

    # ‚úÖ Return the final response
    if success:
        return JSONResponse(content={"message": "Graph loaded successfully", "graph_id": graph_id})

    # Log detailed error but return generic message to user
    logging.error("Graph loading failed: %s", str(result)[:100])
    raise HTTPException(status_code=400, detail="Failed to load graph data")


@graphs_router.post("/{graph_id}")
@token_required
async def query_graph(request: Request, graph_id: str, chat_data: ChatRequest):
    """
    text2sql
    """
    # Input validation
    if not graph_id or not isinstance(graph_id, str):
        raise HTTPException(status_code=400, detail="Invalid graph_id")

    # Sanitize graph_id to prevent injection
    graph_id = graph_id.strip()[:100]  # Limit length and strip whitespace
    if not graph_id:
        raise HTTPException(status_code=400, detail="Invalid graph_id")

    graph_id = request.state.user_id + "_" + graph_id

    queries_history = chat_data.chat if hasattr(chat_data, 'chat') else None
    result_history = chat_data.result if hasattr(chat_data, 'result') else None
    instructions = chat_data.instructions if hasattr(chat_data, 'instructions') else None

    if not queries_history or not isinstance(queries_history, list):
        raise HTTPException(status_code=400, detail="Invalid or missing chat history")

    if len(queries_history) == 0:
        raise HTTPException(status_code=400, detail="Empty chat history")

    logging.info("User Query: %s", sanitize_query(queries_history[-1]))

    # Create a generator function for streaming
    async def generate():
        # Start overall timing
        overall_start = time.perf_counter()
        logging.info("Starting query processing pipeline for query: %s", sanitize_query(queries_history[-1]))
        
        agent_rel = RelevancyAgent(queries_history, result_history)
        agent_an = AnalysisAgent(queries_history, result_history)

        step = {"type": "reasoning_step",
                "message": "Step 1: Analyzing user query and generating SQL..."}
        yield json.dumps(step) + MESSAGE_DELIMITER
        # Ensure the database description is loaded
        db_description, db_url = await get_db_description(graph_id)
        
        # Determine database type and get appropriate loader
        db_type, loader_class = get_database_type_and_loader(db_url)

        if not loader_class:
            overall_elapsed = time.perf_counter() - overall_start
            logging.info("Query processing failed (no loader) - Total time: %.2f seconds", overall_elapsed)
            yield json.dumps({
                "type": "error", 
                "message": "Unable to determine database type"
            }) + MESSAGE_DELIMITER
            return

        # Start both tasks concurrently
        find_task = asyncio.create_task(find(graph_id, queries_history, db_description))

        relevancy_task = asyncio.create_task(agent_rel.get_answer(
            queries_history[-1], db_description
        ))

        logging.info("Starting relevancy check and graph analysis concurrently")
        
        # Wait for relevancy check first
        answer_rel = await relevancy_task
        
        if answer_rel["status"] != "On-topic":
            # Cancel the find task since query is off-topic
            find_task.cancel()
            try:
                await find_task
            except asyncio.CancelledError:
                logging.info("Find task cancelled due to off-topic query")
            
            step = {
                "type": "followup_questions",
                "message": "Off topic question: " + answer_rel["reason"],
            }
            logging.info("SQL Fail reason: %s", answer_rel["reason"])
            yield json.dumps(step) + MESSAGE_DELIMITER
            # Total time for off-topic query
            overall_elapsed = time.perf_counter() - overall_start
            logging.info("Query processing completed (off-topic) - Total time: %.2f seconds", overall_elapsed)
        else:
            # Query is on-topic, wait for find results
            result = await find_task

            logging.info("Calling to analysis agent with query: %s",
                         sanitize_query(queries_history[-1]))

            logging.info("Starting SQL generation with analysis agent")
            answer_an = agent_an.get_analysis(
                queries_history[-1], result, db_description, instructions
            )

            logging.info("Generated SQL query: %s", answer_an['sql_query'])
            yield json.dumps(
                {
                    "type": "final_result",
                    "data": answer_an["sql_query"],
                    "conf": answer_an["confidence"],
                    "miss": answer_an["missing_information"],
                    "amb": answer_an["ambiguities"],
                    "exp": answer_an["explanation"],
                    "is_valid": answer_an["is_sql_translatable"],
                }
            ) + MESSAGE_DELIMITER

            # If the SQL query is valid, execute it using the postgress database db_url
            if answer_an["is_sql_translatable"]:
                # Check if this is a destructive operation that requires confirmation
                sql_query = answer_an["sql_query"]
                sql_type = sql_query.strip().split()[0].upper() if sql_query else ""

                destructive_ops = ['INSERT', 'UPDATE', 'DELETE', 'DROP',
                                  'CREATE', 'ALTER', 'TRUNCATE']
                if sql_type in destructive_ops:
                    # This is a destructive operation - ask for user confirmation
                    confirmation_message = f"""‚ö†Ô∏è DESTRUCTIVE OPERATION DETECTED ‚ö†Ô∏è

The generated SQL query will perform a **{sql_type}** operation:

SQL:
{sql_query}

What this will do:
"""
                    if sql_type == 'INSERT':
                        confirmation_message += "‚Ä¢ Add new data to the database"
                    elif sql_type == 'UPDATE':
                        confirmation_message += ("‚Ä¢ Modify existing data in the "
                                                "database")
                    elif sql_type == 'DELETE':
                        confirmation_message += ("‚Ä¢ **PERMANENTLY DELETE** data "
                                                "from the database")
                    elif sql_type == 'DROP':
                        confirmation_message += ("‚Ä¢ **PERMANENTLY DELETE** entire "
                                                "tables or database objects")
                    elif sql_type == 'CREATE':
                        confirmation_message += ("‚Ä¢ Create new tables or database "
                                                "objects")
                    elif sql_type == 'ALTER':
                        confirmation_message += ("‚Ä¢ Modify the structure of existing "
                                                "tables")
                    elif sql_type == 'TRUNCATE':
                        confirmation_message += ("‚Ä¢ **PERMANENTLY DELETE ALL DATA** "
                                                "from specified tables")
                    confirmation_message += """

‚ö†Ô∏è WARNING: This operation will make changes to your database and may be irreversible.
"""

                    yield json.dumps(
                        {
                            "type": "destructive_confirmation",
                            "message": confirmation_message,
                            "sql_query": sql_query,
                            "operation_type": sql_type
                        }
                    ) + MESSAGE_DELIMITER
                    # Log end-to-end time for destructive operation that requires confirmation
                    overall_elapsed = time.perf_counter() - overall_start
                    logging.info("Query processing halted for confirmation - Total time: %.2f seconds", overall_elapsed)
                    return  # Stop here and wait for user confirmation

                try:
                    step = {"type": "reasoning_step", "message": "Step 2: Executing SQL query"}
                    yield json.dumps(step) + MESSAGE_DELIMITER

                    # Check if this query modifies the database schema using the appropriate loader
                    is_schema_modifying, operation_type = (
                        loader_class.is_schema_modifying_query(sql_query)
                    )

                    query_results = loader_class.execute_sql_query(answer_an["sql_query"], db_url)
                    
                    yield json.dumps(
                        {
                            "type": "query_result",
                            "data": query_results,
                        }
                    ) + MESSAGE_DELIMITER

                    # If schema was modified, refresh the graph using the appropriate loader
                    if is_schema_modifying:
                        step = {"type": "reasoning_step",
                               "message": ("Step 3: Schema change detected - "
                                         "refreshing graph...")}
                        yield json.dumps(step) + MESSAGE_DELIMITER

                        refresh_result = await loader_class.refresh_graph_schema(
                            graph_id, db_url)
                        refresh_success, refresh_message = refresh_result

                        if refresh_success:
                            refresh_msg = (f"‚úÖ Schema change detected "
                                         f"({operation_type} operation)\n\n"
                                         f"üîÑ Graph schema has been automatically "
                                         f"refreshed with the latest database "
                                         f"structure.")
                            yield json.dumps(
                                {
                                    "type": "schema_refresh",
                                    "message": refresh_msg,
                                    "refresh_status": "success"
                                }
                            ) + MESSAGE_DELIMITER
                        else:
                            failure_msg = (f"‚ö†Ô∏è Schema was modified but graph "
                                         f"refresh failed: {refresh_message}")
                            yield json.dumps(
                                {
                                    "type": "schema_refresh",
                                    "message": failure_msg,
                                    "refresh_status": "failed"
                                }
                            ) + MESSAGE_DELIMITER

                    # Generate user-readable response using AI
                    step_num = "4" if is_schema_modifying else "3"
                    step = {"type": "reasoning_step",
                           "message": f"Step {step_num}: Generating user-friendly response"}
                    yield json.dumps(step) + MESSAGE_DELIMITER

                    response_agent = ResponseFormatterAgent()
                    user_readable_response = response_agent.format_response(
                        user_query=queries_history[-1],
                        sql_query=answer_an["sql_query"],
                        query_results=query_results,
                        db_description=db_description
                    )

                    yield json.dumps(
                        {
                            "type": "ai_response",
                            "message": user_readable_response,
                        }
                    ) + MESSAGE_DELIMITER

                    # Log overall completion time
                    overall_elapsed = time.perf_counter() - overall_start
                    logging.info("Query processing completed successfully - Total time: %.2f seconds", overall_elapsed)

                except Exception as e:
                    overall_elapsed = time.perf_counter() - overall_start
                    logging.error("Error executing SQL query: %s", str(e))
                    logging.info("Query processing failed during execution - Total time: %.2f seconds", overall_elapsed)
                    yield json.dumps(
                        {"type": "error", "message": "Error executing SQL query"}
                    ) + MESSAGE_DELIMITER
            else:
                # SQL query is not valid/translatable
                overall_elapsed = time.perf_counter() - overall_start
                logging.info("Query processing completed (non-translatable SQL) - Total time: %.2f seconds", overall_elapsed)

        # Log timing summary at the end of processing
        overall_elapsed = time.perf_counter() - overall_start
        logging.info("Query processing pipeline completed - Total time: %.2f seconds", overall_elapsed)

    return StreamingResponse(generate(), media_type="application/json")


@graphs_router.post("/{graph_id}/confirm")
@token_required
async def confirm_destructive_operation(
    request: Request,
    graph_id: str,
    confirm_data: ConfirmRequest,
):
    """
    Handle user confirmation for destructive SQL operations
    """
    graph_id = request.state.user_id + "_" + graph_id.strip()

    if hasattr(confirm_data, 'confirmation'):
        confirmation = confirm_data.confirmation.strip().upper()
    else:
        confirmation = ""

    sql_query = confirm_data.sql_query if hasattr(confirm_data, 'sql_query') else ""
    queries_history = confirm_data.chat if hasattr(confirm_data, 'chat') else []

    if not sql_query:
        raise HTTPException(status_code=400, detail="No SQL query provided")

    # Create a generator function for streaming the confirmation response
    async def generate_confirmation():
        if confirmation == "CONFIRM":
            try:
                db_description, db_url = await get_db_description(graph_id)

                # Determine database type and get appropriate loader
                db_type, loader_class = get_database_type_and_loader(db_url)

                if not loader_class:
                    yield json.dumps({
                        "type": "error", 
                        "message": "Unable to determine database type"
                    }) + MESSAGE_DELIMITER
                    return

                step = {"type": "reasoning_step",
                       "message": "Step 2: Executing confirmed SQL query"}
                yield json.dumps(step) + MESSAGE_DELIMITER

                # Check if this query modifies the database schema using appropriate loader
                is_schema_modifying, operation_type = (
                    loader_class.is_schema_modifying_query(sql_query)
                )
                query_results = loader_class.execute_sql_query(sql_query, db_url)
                yield json.dumps(
                    {
                        "type": "query_result",
                        "data": query_results,
                    }
                ) + MESSAGE_DELIMITER

                # If schema was modified, refresh the graph
                if is_schema_modifying:
                    step = {"type": "reasoning_step",
                           "message": "Step 3: Schema change detected - refreshing graph..."}
                    yield json.dumps(step) + MESSAGE_DELIMITER

                    refresh_success, refresh_message = (
                        await loader_class.refresh_graph_schema(graph_id, db_url)
                    )

                    if refresh_success:
                        yield json.dumps(
                            {
                                "type": "schema_refresh",
                                "message": (f"‚úÖ Schema change detected ({operation_type} "
                                          "operation)\n\nüîÑ Graph schema has been automatically "
                                          "refreshed with the latest database structure."),
                                "refresh_status": "success"
                            }
                        ) + MESSAGE_DELIMITER
                    else:
                        yield json.dumps(
                            {
                                "type": "schema_refresh",
                                "message": (f"‚ö†Ô∏è Schema was modified but graph refresh failed: "
                                          f"{refresh_message}"),
                                "refresh_status": "failed"
                            }
                        ) + MESSAGE_DELIMITER

                # Generate user-readable response using AI
                step_num = "4" if is_schema_modifying else "3"
                step = {"type": "reasoning_step",
                       "message": f"Step {step_num}: Generating user-friendly response"}
                yield json.dumps(step) + MESSAGE_DELIMITER

                response_agent = ResponseFormatterAgent()
                user_readable_response = response_agent.format_response(
                    user_query=queries_history[-1] if queries_history else "Destructive operation",
                    sql_query=sql_query,
                    query_results=query_results,
                    db_description=db_description
                )

                yield json.dumps(
                    {
                        "type": "ai_response",
                        "message": user_readable_response,
                    }
                ) + MESSAGE_DELIMITER

            except Exception as e:
                logging.error("Error executing confirmed SQL query: %s", str(e))
                yield json.dumps(
                    {"type": "error", "message": "Error executing query"}
                ) + MESSAGE_DELIMITER
        else:
            # User cancelled or provided invalid confirmation
            yield json.dumps(
                {
                    "type": "operation_cancelled",
                    "message": "Operation cancelled. The destructive SQL query was not executed."
                }
            ) + MESSAGE_DELIMITER

    return StreamingResponse(generate_confirmation(), media_type="application/json")


@graphs_router.post("/{graph_id}/refresh")
@token_required
async def refresh_graph_schema(request: Request, graph_id: str):
    """
    Manually refresh the graph schema from the database.
    This endpoint allows users to manually trigger a schema refresh
    if they suspect the graph is out of sync with the database.
    """
    graph_id = request.state.user_id + "_" + graph_id.strip()

    try:
        # Get database connection details
        _, db_url = await get_db_description(graph_id)

        if not db_url or db_url == "No URL available for this database.":
            return JSONResponse({
                "success": False,
                "error": "No database URL found for this graph"
            }, status_code=400)

        # Determine database type and get appropriate loader
        db_type, loader_class = get_database_type_and_loader(db_url)

        if not loader_class:
            return JSONResponse({
                "success": False,
                "error": "Unable to determine database type"
            }, status_code=400)

        # Perform schema refresh using the appropriate loader
        success, message = await loader_class.refresh_graph_schema(graph_id, db_url)

        if success:
            return JSONResponse({
                "success": True,
                "message": f"Graph schema refreshed successfully using {db_type}"
            })

        logging.error("Schema refresh failed for graph %s: %s", graph_id, message)
        return JSONResponse({
            "success": False,
            "error": "Failed to refresh schema"
        }, status_code=500)

    except Exception as e:
        logging.error("Error in manual schema refresh: %s", e)
        return JSONResponse({
            "success": False,
            "error": "Error refreshing schema"
        }, status_code=500)
